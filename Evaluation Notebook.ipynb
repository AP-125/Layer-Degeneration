{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dcc91d-cef8-4d47-9e36-3a64a28644df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install huggingface_hub\n",
    "%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install evaluate\n",
    "\n",
    "from huggingface_hub import snapshot_download, login\n",
    "import os\n",
    "\n",
    "token = \"\"\n",
    "#Your HF token.\n",
    "login(token=token)\n",
    "\n",
    "local_dir = snapshot_download(\"google/gemma-3-12b-it\")\n",
    "print(\"Files are in:\", local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7e82b2-e637-46b1-9d1b-46b0d29ab069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch, copy\n",
    "\n",
    "def reset_weights(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n",
    "model_path = \"/root/.cache/huggingface/hub/models--google--gemma-3-12b-it/snapshots/96b6f1eccf38110c56df3a15bffe176da04bfd80\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(\"cuda\").eval()\n",
    "model.config.use_cache = False\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "new_layer = copy.deepcopy(model.model.language_model.layers[0]).to(\"cuda\").eval()\n",
    "#new_layer = copy.deepcopy(model.model.language_model.layers[23]).to(\"cuda\").eval()\n",
    "#new_layer = copy.deepcopy(model.model.language_model.layers[47]).to(\"cuda\").eval()\n",
    "\n",
    "'''\n",
    "Zeroed out\n",
    "'''\n",
    "#excluded = set(new_layer.post_attention_layernorm.parameters()) | set(new_layer.post_feedforward_layernorm.parameters())\n",
    "#\n",
    "#with torch.no_grad():\n",
    "#    for p in new_layer.parameters():\n",
    "#        if p not in excluded:\n",
    "#            p.data.zero_()\n",
    "\n",
    "'''\n",
    "Randomly re-initialized\n",
    "'''\n",
    "#def reset_weights(m):\n",
    "#    if hasattr(m, 'reset_parameters'):\n",
    "#        m.reset_parameters()\n",
    "#with torch.no_grad():\n",
    "#    new_layer.apply(reset_weights)\n",
    "\n",
    "'''\n",
    "Briefly re-trained\n",
    "'''\n",
    "#ckpt_path = \"retrained_layer_0.pt\"\n",
    "#ckpt_path = \"retrained_layer_23.pt\"\n",
    "#with torch.no_grad():\n",
    "#    state_dict = torch.load(ckpt_path, map_location=\"cuda\")[\"model_state_dict\"]\n",
    "#    new_layer.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.model.language_model.layers[0] = new_layer\n",
    "#    model.model.language_model.layers[23] = new_layer\n",
    "#    model.model.language_model.layers[47] = new_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc07c908-905f-4533-b843-ce8ea7d693f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell a story:\"\n",
    "#prompt = \"Factor x^2+3x+2:\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "gen_ids = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(gen_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d33396-6141-491a-b3e1-5eb9368baa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "\n",
    "ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "#ds = load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
    "texts = [\"\\n\".join(ds[\"text\"])]\n",
    "\n",
    "ppl = evaluate.load(\"perplexity\", module_type=\"measurement\")\n",
    "\n",
    "save_dir = \"model_temp_save\"\n",
    "model.save_pretrained(save_dir); tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = ppl.compute(model_id=save_dir, data=texts, add_start_token=True, batch_size=8, max_length=512)\n",
    "\n",
    "print(\"PPL:\", res[\"mean_perplexity\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
