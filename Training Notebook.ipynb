{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492bbc44-eaca-4f3a-9f48-79289207a05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.32.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface_hub) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->huggingface_hub) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: 'datasets,': Expected end or semicolon (after name and no valid version specifier)\n",
      "    datasets,\n",
      "            ^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/nightly/cu128\n",
      "Requirement already satisfied: torch in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0.dev20250604+cu128)\n",
      "Requirement already satisfied: torchvision in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.23.0.dev20250605+cu128)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0.dev20250605+cu128)\n",
      "Requirement already satisfied: filelock in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: transformers in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.32.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\klgtt\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.4.26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub\n",
    "%pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128\n",
    "%pip install transformers\n",
    "%pip install datasets, evaluate\n",
    "\n",
    "from huggingface_hub import snapshot_download, login\n",
    "import os\n",
    "\n",
    "token = \"\"\n",
    "#Your HF token.\n",
    "login(token=token)\n",
    "\n",
    "local_dir = snapshot_download(\"google/gemma-3-12b-it\")\n",
    "print(\"Files are in:\", local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae9329-f4fe-421f-b490-93af50013270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_path = \"/root/.cache/huggingface/hub/models--google--gemma-3-12b-it/snapshots/96b6f1eccf38110c56df3a15bffe176da04bfd80\"\n",
    "#Or whatever path the model was downloaded to.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac734573-3058-49d9-8c2b-d8b2820939bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cuda\").eval()\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "#We're not actually changing anything in the model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd60c6-c168-445a-bac7-8997a682e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model.model.vision_tower)\n",
    "del(model.model.multi_modal_projector)\n",
    "\n",
    "lm = model.model.language_model\n",
    "layer_list = []\n",
    "for i in range(24):\n",
    "##for i in range(1):\n",
    "    layer_list.append(lm.layers[i])\n",
    "lm.layers = torch.nn.ModuleList(layer_list)\n",
    "\n",
    "del(model.lm_head)\n",
    "#Keeping things tidy, delete everything we don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4415f59c-0431-4e84-bbb6-cd2d952a996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def reset_weights(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n",
    "new_layer = copy.deepcopy(model.model.language_model.layers[23]).to(\"cuda\")\n",
    "##new_layer = copy.deepcopy(model.model.language_model.layers[0]).to(\"cuda\")\n",
    "#Not that it really matters.\n",
    "new_layer.config.use_cache = False\n",
    "#Turn off KV caching for training.\n",
    "new_layer.apply(reset_weights)\n",
    "#No cheating!\n",
    "for param in new_layer.parameters():\n",
    "    param.requires_grad = True\n",
    "ckpt_path = \"epoch_04_rd2.pt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cuda\")\n",
    "with torch.no_grad():\n",
    "    new_layer.load_state_dict(checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf31a13-cf4b-4858-b630-814451f4e10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decay, no_decay = [], []\n",
    "for n, p in new_layer.named_parameters():\n",
    "    if p.ndim > 1:\n",
    "        decay.append(p)\n",
    "    else:\n",
    "        no_decay.append(p)\n",
    "\n",
    "optim = torch.optim.AdamW([{\"params\": decay, \"weight_decay\": 0.1}, {\"params\": no_decay,  \"weight_decay\": 0.0}], lr=3e-4)\n",
    "#Weight decay apparently works best on matrix parameters and not scalar parameters like those that appear in layernorm.\n",
    "#I'm a little skeptical that this really matters, but sure, we'll play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25d899a-1175-4534-a134-9b1e4468cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_latent = None\n",
    "gold_latent = None\n",
    "\n",
    "def hook_fn(mod, args, kwargs, output):\n",
    "    #We're going to attach this hook to the layer we're copying.\n",
    "    #We take its input, feed it into the original layer to get the gold output, and feed it into our copied layer to get the predicted output.\n",
    "    global pred_latent, gold_latent\n",
    "\n",
    "    inp = args[0]\n",
    "    peg = kwargs[\"position_embeddings_global\"]\n",
    "    pel = kwargs[\"position_embeddings_local\"]\n",
    "    #Gemma really likes its position embeddings.\n",
    "\n",
    "    attn_m = kwargs.get(\"attention_mask\", None)\n",
    "    pos_ids = kwargs.get(\"position_ids\",    None)\n",
    "\n",
    "    pred_latent = new_layer(\n",
    "        inp, peg, pel,\n",
    "        attention_mask=attn_m,\n",
    "        position_ids=pos_ids,\n",
    "        use_cache=False,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    )[0]\n",
    "\n",
    "    gold_latent = output[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc96e96-8627-4895-84cf-45b035307873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b63a6-3390-43a0-94af-6b6afa4b24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "from datasets import interleave_datasets\n",
    "\n",
    "wiki   = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\",  split=\"train\", streaming=True)\n",
    "books  = load_dataset(\"lucadiliello/bookcorpusopen\", split=\"train\", streaming=True)\n",
    "crawl  = load_dataset(\"allenai/c4\", \"en\", split=\"train\", streaming=True)\n",
    "#A decent enough mix for a quick run, I think.\n",
    "\n",
    "mix = interleave_datasets([wiki, books, crawl], probabilities=[0.3, 0.3, 0.4])\n",
    "ds_tok = mix.map(tokenize, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcf5b38-f35e-4cbd-928b-42c6d325edd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(batch, chunk=2048):\n",
    "    #Grouping the data into chunks of size 2048.\n",
    "    #So we're only training on a context size of 2048 for this quick run.\n",
    "    tokens = sum(batch[\"input_ids\"], [])\n",
    "    #Flattens batch[\"input_ids\"] (a list of lists) into one long list.\n",
    "    n_full = len(tokens) // chunk * chunk\n",
    "    return {\"input_ids\": [tokens[i : i+chunk] for i in range(0, n_full, chunk)]}\n",
    "    #Batches back into multiples of chunk.\n",
    "\n",
    "ds_lm = ds_tok.map(group_texts, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d00932-f219-47cc-be38-b5e27396b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import IterableDataset\n",
    "\n",
    "class TokenCappedDataset(IterableDataset):\n",
    "    def __init__(self, base_ds, max_tokens: int):\n",
    "        self.base_ds = base_ds\n",
    "        self.max_tokens = max_tokens\n",
    "        #We're certainly not going to go through the full datasets, so we'll cut off after a fixed number of tokens and call that an epoch.\n",
    "\n",
    "    def __iter__(self):\n",
    "        token_budget = self.max_tokens\n",
    "        for ex in self.base_ds:\n",
    "            tok_len = len(ex[\"input_ids\"])\n",
    "            if tok_len > token_budget:\n",
    "                break\n",
    "            token_budget -= tok_len\n",
    "            yield ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfecc3-4d9e-4ba0-9ea4-ba3ecbbf2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "class CausalLMCollator:\n",
    "    def __init__(self, tokenizer, pad_to_multiple_of = 8):\n",
    "        self.tok = tokenizer\n",
    "        self.pad_multiple = pad_to_multiple_of\n",
    "\n",
    "    def __call__(self, examples: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        ids = [e[\"input_ids\"] for e in examples]\n",
    "\n",
    "        batch = self.tok.pad({\"input_ids\": ids}, padding=\"longest\", pad_to_multiple_of=self.pad_multiple, return_tensors=\"pt\")\n",
    "        return batch\n",
    "        #Don't need \"labels\" in batch because we're not using Trainer; we're computing a custom loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cfb608-efce-4f24-9c77-093f49405a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "collator = CausalLMCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c060451-c699-45d1-8159-16a5fee4165e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup\n",
    "\n",
    "tokens_per_epoch = 100_000_000\n",
    "num_epochs = 20\n",
    "#I never hit the full 20 epochs since the loss function always seemed to converge comfortably before that.\n",
    "#Also because training costs money.\n",
    "\n",
    "handle = model.model.language_model.layers[23].register_forward_hook(hook_fn, with_kwargs=True)\n",
    "##handle = model.model.language_model.layers[0].register_forward_hook(hook_fn, with_kwargs=True)\n",
    "new_layer.train()\n",
    "\n",
    "tokens_seen = 0\n",
    "tokens_per_batch = 8 * 2048\n",
    "steps_per_epoch = math.ceil(tokens_per_epoch / tokens_per_batch)\n",
    "\n",
    "total_steps = num_epochs * steps_per_epoch\n",
    "warmup_steps = int(0.05 * total_steps)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_ds = TokenCappedDataset(ds_lm, tokens_per_epoch)\n",
    "    loader = DataLoader(epoch_ds, batch_size=8, collate_fn=collator)\n",
    "\n",
    "    pbar = tqdm(total=steps_per_epoch, desc=f\"Epoch {epoch:02d}\", leave=False)\n",
    "    epoch_loss_sum = 0.0\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "        _ = model.model.language_model(**batch)\n",
    "        #The hook takes care of everything.\n",
    "\n",
    "        diff = pred_latent - gold_latent\n",
    "        loss = diff.norm()\n",
    "        #An L2-norm loss on the latents.\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        epoch_loss_sum += loss.item()\n",
    "        pbar.update(1)\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\") \n",
    "\n",
    "        tokens_seen += tokens_per_batch\n",
    "\n",
    "    pbar.close()\n",
    "    avg_loss = epoch_loss_sum / steps_per_epoch\n",
    "    print(f\"Epoch {epoch}: Average Loss {avg_loss}\")\n",
    "\n",
    "    tokens_seen = 0\n",
    "\n",
    "    ckpt_path = f\"epoch_{epoch:02d}.pt\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": new_layer.state_dict(),\n",
    "            \"optimizer_state_dict\": optim.state_dict(),\n",
    "            \"avg_loss\": avg_loss\n",
    "        },\n",
    "        ckpt_path\n",
    "    )\n",
    "\n",
    "handle.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
